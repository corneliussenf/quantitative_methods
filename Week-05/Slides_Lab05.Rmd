---
title: "Statistics in R"
author: "Cornelius Senf and Dirk Pflugmacher"
date: ""
output:
  ioslides_presentation:
    smaller: yes
    widescreen: yes
  beamer_presentation: default
  slidy_presentation: default
subtitle: Quantitative Methods - labs Session 05
header-includes: \usepackage{amsmath}
---

<style type="text/css">
slide.backdrop {
  background: none !important;
  background-color: white !important;
}
</style>

```{r echo=FALSE, hide=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
```

## Today's session:

- The lm() function and its extractor functions
- Plotting regression lines
- Prediction versus confidence bands
- Model diagnostics with R

# The `lm()` function

## The `lm()` function

The `lm()` function implements simple linear regression in R. The argument to `lm()` is a model formula in which the tilde symbol (~) should be read as "described by". 

```{r}
dat <- read.csv("Data/anscombe/anscombe01.txt")

lm.anscombe1 <- lm(y ~ x, data = dat)
```

The result of `lm()` is a model object. This is a distinctive concept of R. Whereas other statistical systems focus on generating printed output that can be controlled by setting options, you get instead the result of a model fit encapsulated in an object from which the desired quantities can be obtained using **extractor functions**. An `lm()` object does in fact contain much more information than you see when it is printed.

## The `lm()` function

```{r}
lm.anscombe1
```

In its raw form, the output of `lm()` is very brief. All you see is the estimated intercept and the estimated slope of the regression. Those estimates are the same you calculated by hand!

## The `lm()` function

Using the `summary()` function we can get a more comprehensive summary:

```{r}
summary(lm.anscombe1)
```

## The `lm()` and `anova()` function

Calling `anova()` on an object returned by `lm()` computes the ANOVA tables.

```{r}
anova(lm.anscombe1)
```

## `lm()` and its extractor functions

The function `fitted()` returns fitted values: the y-values that you would expect for the given x-values according to the best-fitting straight line. The residuals shown by `residuals()` is the difference between this and the observed y.

```{r}
fitted(lm.anscombe1)
residuals(lm.anscombe1)
```

## `lm()` and its extractor functions

The function `coefficients()` returns the estimated model parameters; in this case intercept and slope.

```{r}
coefficients(lm.anscombe1)
```

You can extract both parameters using simple vector indexing:

```{r}
intercept <- coefficients(lm.anscombe1)[1]
slope <- coefficients(lm.anscombe1)[2]
```

## Plot the fitted line

```{r, echo=TRUE, results='asis', warning=FALSE, message=FALSE, fig.height=3, fig.width=3, fig.align='center'}
dat$fitted <- fitted(lm.anscombe1)

ggplot(dat, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = fitted), col = "red")
```

## Plot the fitted line

```{r, echo=TRUE, results='asis', warning=FALSE, message=FALSE, fig.height=3, fig.width=3, fig.align='center'}
parameters <- coefficients(lm.anscombe1)

ggplot(dat, aes(x = x, y = y)) +
  geom_point() +
  geom_abline(intercept = parameters[1], slope = parameters[2], col="red")
```

## `lm()` and its extractor functions

Predicted values can be extracted with the function `predict()`. With no arguments, it just gives the fitted values:

```{r}
predict(lm.anscombe1) # Similar to fitted(lm.anscombe1)
```

You can also predict outcomes for new values:

```{r}
new.dat <- data.frame(x = c(1, 4, 6, 19))
predict(lm.anscombe1, newdata = new.dat)
```

Please note that the data frame must have the same column names as used in the formula within `lm()`!

## `lm()` and its extractor functions

Setting the argument `interval = "confidence"` you get the vector of predicted values with a lower and upper limit given **uncertainty in the parameters**. 

```{r}
predict(lm.anscombe1, interval = 'confidence')
```

Note that R by default returns the 95% confidence interval. You can change it by seting the argument `level` to another confidence level (e.g., `level = 0.99`).

## `lm()` and its extractor functions

The extractor functions also work within `dplyr` pipes!

```{r}
lm.anscombe1 %>% 
  predict(., interval = 'confidence')
```

## Plot uncertainty bands with ggplot

```{r}
prediction.anscombe1 <- lm.anscombe1 %>%
  predict(., interval = 'confidence') %>%
  as.data.frame() %>%
  mutate(x = dat$x)

p <- ggplot(dat, aes(x = x, y = y)) +
  geom_point() +
  geom_line(data = prediction.anscombe1, aes(x = x, y = fit)) +
  geom_line(data = prediction.anscombe1, aes(x = x, y = upr), linetype = "dashed" ) +
  geom_line(data = prediction.anscombe1, aes(x = x, y = lwr), linetype = "dashed")
```

## Plot confidence bands with ggplot

```{r, echo=FALSE, fig.width=4, fig.height=4, fig.align='center'}
print(p)
```

Interpret the confidence band as: When repeating the experiment, we would expect that the estimated regression lines lie within the confidence band 95% of the times.

## Plot prediction bands with ggplot

If you add `interval = "prediction"`, then you get the vector of predicted values with a lower and upper limit given **uncertainty in the parameters** plus **residual variance**. 

```{r, fig.height=3, fig.width=3, fig.align='center'}
prediction.anscombe1 <- lm.anscombe1 %>%
  predict(., interval = 'prediction') %>%
  as.data.frame() %>%
  mutate(x = dat$x)

p <- ggplot(dat, aes(x = x, y = y)) +
  geom_point() +
  geom_line(data = prediction.anscombe1, aes(x = x, y = fit)) +
  geom_line(data = prediction.anscombe1, aes(x = x, y = upr), linetype = "dashed" ) +
  geom_line(data = prediction.anscombe1, aes(x = x, y = lwr), linetype = "dashed")
```

## Plot prediction bands with ggplot

```{r, echo=FALSE, fig.width=4, fig.height=4, fig.align='center'}
print(p)
```

Interpret the prediction band as: When predicting new values, we would expect that 95% of them lie within the prediction band.

## Meaning of the prediction band

We can simulate 10,000 new $x$ values by drawing from a random normal distribution. Using those simulated values, we can predict new $y$ values. However, we need to consider not only the uncertainty in the regression coefficients, but also in the model fit (i.e., how variable are the data around the regression line). We can estimate the residual variance using the argument `se.fit = TRUE` in the `predict()` function:

```{r}
random.data <- data.frame(x = rnorm(10000, mean = mean(dat$x), sd = sd(dat$x)))
prediction <- predict(lm.anscombe1, newdata = random.data, se.fit = TRUE)

names(prediction)
```

## Meaning of the prediction band

The function returns a list with four elements: 

1) The fitted values

2) the standard error of each fitted value (considering parameter uncertainty)

3) the degrees of freedom: df = n - 2

4) the residual variance $sigma^2$: $y = b0 + b1 * x + N(0, sigma^2)$

## Meaning of the prediction band

```{r}
head(prediction$fit) # Fitted values

head(prediction$se.fit) # Standard error (parameter uncertainty)

prediction$df # Degrees of freedom: df = (n - 2)

prediction$residual.scale # Residual variance sigma^2: y = b0 + b1 * x + N(0, sigma^2)
```

## Meaning of the prediction band

Using the residual variance we can now predict expected outcomes based on the 10,000 simulated values:

```{r}
random.data$predicted <- prediction$fit + rnorm(10000, mean = 0, sd = prediction$residual.scale)

p <- ggplot(prediction.anscombe1) +
  geom_line(aes(x = x, y = upr)) +
  geom_line(aes(x = x, y = lwr)) +
  geom_point(data = random.data, aes(x = x, y = predicted), col = "red", alpha = 0.2) +
  labs(x = "x", y = "prediction")
```

## Meaning of the prediction band

```{r echo=FALSE, fig.width=4, fig.height=4, fig.align='center'}
print(p)
```

## Plotting linear models in ggplot using `geom_smooth()`

An alternative to first fitting the model and following plotting the data is to use `geom_smooth()`:

```{r}
p <- ggplot(dat, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, level = 0.95)
```

## Plotting linear models in ggplot using `geom_smooth()`

```{r, echo=FALSE, fig.width=4, fig.height=4, fig.align='center'}
print(p)
```

Note that `geom_smooth()` with `se = TRUE` plots the 95% confidence band, not the prediction band! You can change the confidence level using the `level` argument.

# Model diagnostics

## Five-step plan to diagnose your model

The validity of a model depends on several assumptions regarding the distribution of residuals and the functional relationship between $y$ and $x$. Visual/numerical model diagnostics should cover the following aspects:

1) Analysis of the distribution of the predictor variable

2) Analysis of leverage points (points with high influence)

3) Analysis of the dispersion

4) Analysis of the residuals (independence, normal distribution, and homoscedasticity)

5) Analysis of the functional relationship between response and predictor

## Anscombe data

```{r}
anscombe.all <- read.csv("Data/anscombe_all.csv")

p <- ggplot(anscombe.all, aes(x = x, y = y)) +
            geom_point(alpha = 0.5) +
            geom_smooth(method = "lm", se = FALSE, col = 'red') +      
            facet_wrap(~ dataset)

```

## Anscombe data

```{r, echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
print(p)
```

## 1) The distribution of the predictor variable

- Extreme $x$ values can dominate a regression.

- Ideally, $x$ would be uniformly distributed. We make no explicit assumptions on the distribution of $x$, but it can matter.

- This should be considered when designing experiments and sampling surveys.

- Log- and squareroot-transformations can dampen the effect of extreme values (but be careful as the interpretation changes!).

## 1) The distribution of the predictor variable

```{r fig.height=4, fig.width=4, fig.align='center'}
ggplot(anscombe.all, aes(x = dataset, y = x)) +
            geom_boxplot()

```

## 2) Leverage points

There is no practical justification to eliminate 'outliers' just because they are extreme values. Removing extreme values has to be well justified, e.g. measurement error. 

Although, you should not simply delete outliers, you can run your analysis twice (with and without outliers) to test the robustness of the analysis.

Cook's distance is a common measure to identify influencial points. Values greater than 1 or greater than 4/n (n = number of samples) are candidate influential points. You can extract Cook's distance using the `cooks.distance()`, though we suggest to rely rather on your intuition than on strict thresholds.

## 3) Dispersion

- Dispersion quantifies, how much higher (or lower) the observed variance is relative to the expected variance of the model (overdispersion or underdispersion).

- This is not relevant for the normal distribution case (OLS assumes error term is normally distributed), since the standard deviation (measure of variability) is independent of the mean, and hence, estimated separately.

- But dispersion will play a role for other distributions modeled with Generalized Linear Models.

- ...to be continued.

## 4) Response residuals

The residuals should be independent of the fitted values. We can check this by plotting the residuals against the fitted values:

```{r, fig.show='hide'}
lm.anscombe1 <- anscombe.all %>% filter(dataset == "Anscombe 1") %>% lm(y ~ x, data = .)

diagnostics.anscombe1 <- data.frame(residuals = residuals(lm.anscombe1),
                                    fitted = fitted(lm.anscombe1))

p1 <- ggplot(data = diagnostics.anscombe1, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  labs(title = "Anscombe 1")

lm.anscombe2 <- anscombe.all %>% filter(dataset == "Anscombe 2") %>% lm(y ~ x, data = .)

diagnostics.anscombe2 <- data.frame(residuals = residuals(lm.anscombe2),
                                    fitted = fitted(lm.anscombe2))

p2 <- ggplot(data = diagnostics.anscombe2, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  labs(title = "Anscombe 2")
```

## 4) Response residuals

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=9, fig.align='center'}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

## 4) Response residuals

There should be no serial correlation between residuals. We can check this by plotting the residuals over observations:

```{r, fig.show='hide'}
diagnostics.anscombe1 <- diagnostics.anscombe1 %>% mutate(observation = 1:11)

p1 <- ggplot(data = diagnostics.anscombe1, aes(x = observation, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  labs(title = "Anscombe 1")


diagnostics.anscombe2 <- diagnostics.anscombe2 %>% mutate(observation = 1:11)

p2 <- ggplot(data = diagnostics.anscombe2, aes(x = observation, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  labs(title = "Anscombe 2")
```

## 4) Response residuals

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=9, fig.align='center'}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Please note that regression models might have temporally or spatially correlated residuals. In fact, both are very common with ecological data. You will learn how to deal with spatial correlation structure in the residuals later in the course.

## 4) Response residuals

The residuals should be normally distributed, which we can test by plotting the residuals via a histogram

```{r, fig.show='hide'}
p1 <- ggplot(data = diagnostics.anscombe1, aes(x = residuals)) +
  geom_histogram() +
  labs(title = "Anscombe 1")

p2 <- ggplot(data = diagnostics.anscombe2, aes(x = residuals)) +
  geom_histogram() +
  labs(title = "Anscombe 2")
```

## 4) Response residuals

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=9, fig.align='center'}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

With few data, however, histograms are difficult to assess! We hence often prefer QQ-Plots.

## 4) Response residuals

The Quantile-Quantile Plot (QQ-Plot) plots the quantiles of the z-transformed data set over the quantiles of a standard normal distribution:

```{r, echo=T, fig.show='hide'}
diagnostics.anscombe1 <- diagnostics.anscombe1 %>% 
  mutate(residuals_ztrans = (residuals - mean(residuals)) / sd(residuals))

p1 <- ggplot(diagnostics.anscombe1, aes(sample = residuals_ztrans)) +
  stat_qq() +
  geom_abline(intercept = 0, slope = 1)

diagnostics.anscombe3 <- anscombe.all %>%
  filter(dataset == "Anscombe 3") %>%
  lm(y ~ x, data = .) %>% 
  residuals(.) %>%
  data.frame(residuals = .) %>% 
  mutate(residuals_ztrans = (residuals - mean(residuals)) / sd(residuals))

p3 <- ggplot(diagnostics.anscombe3, aes(sample = residuals_ztrans)) +
  stat_qq() +
  geom_abline(intercept = 0, slope = 1)
```



## 4) Response residuals

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=9, fig.align='center'}
gridExtra::grid.arrange(p1, p3, ncol = 2)
```

## 4) Response residuals

If the two datasets come from the same distribution (the residuals are thus normally distributed), the points should lie roughly on a line through the origin with slope 1. Interpretation takes some experience and is not always easy, especially with small sample sizes. Yet, it is one of the best methods! Try https://xiongge.shinyapps.io/QQplots/ to see the effect of deviations from normality on the QQ-plot.

## 5) Functional relationship between response and predictor

```{r, echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=5, fig.width=5, fig.align='center'}
ggplot(anscombe.all, aes(x = x, y = y)) +
  geom_point(alpha=0.5) +
  geom_smooth(method = "lm", se = FALSE, col = 'red', lwd = 0.5) +
  facet_wrap(~dataset, scales = "free")
```

When fitting a linear regression model, we assume a linear relationship between $y$ and $x$. But this assumption may be incorrect. To test whether an assumed relationship matches the data or not, you may need to compare different models (including data transformations). You should also apply your mechanistic understanding of the system you are trying to analyse.
