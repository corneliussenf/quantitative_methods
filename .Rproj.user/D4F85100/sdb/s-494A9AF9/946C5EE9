{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Recap statistics'\nauthor: \"Cornelius Senf\"\ndate: \"cornelius.senf@geo.hu-berlin.de\"\noutput:\n  ioslides_presentation:\n    widescreen: yes\nruntime: html\n---\n\n```{r setup, include=FALSE}\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(gridExtra)\nknitr::opts_chunk$set(dev = 'pdf')\n```\n\n## Syllabus\n\nWe will today recap the following topics, which should be known to successfully participate in the module **Quantitative Methods**:\n\n- Probability\n- Random variable, sample, and sampling\n- Probability distributions\n- Null hypothesis significance testing\n- Correlation\n\n# Probability\n\n## Probability\n\nProbability is a measure of the likelihood that an event $a$ will occur, given all possible events $A$:\n\n$P(a) = \\frac{\\text{Count of a}}{\\text{Count of A}}$\n\n**Example:** The probability of getting a six when wolling a dice once is 1/6.\n\n## Probability\n\nWe can also calculate the probability that event $a$ or event $b$ will occur, given they are mutually exclusive, by adding their individual probabilities:\n\n$P(a \\cap b) = P(a) + P(b)$\n\n**Example:** The probability of getting a six or a three when wolling a dice once is 1/6 + 1/6 = 1/3\n\n## Probability\n\nWhen interested in the probability that event $a$ will occur and then event $b$, given that the events are independent, we calculate the probability by multiplying the individual probabilities:\n\n$P(a \\cup b) = P(a) * P(b)$\n\n**Example:** The probability of getting two times a six when rolling a dice two times is 1/6 * 1/6 = 1/36\n\n## Conditional probabilities\n\nAssume we have an urn with 30 balls in it. Of the 30 balls, 10 are wooden and 20 are made out of plastic. From the 20 balls made out of plastic, 8 are of red color and 12 are of blue color. From the wooden balls, 4 are of red color and 6 are of blue color.\n\nWhat is the probability of drawing ...\n\n- a red ball $P(R)$\n- a wooden ball $P(W)$\n\nMoreover, what is the probability of drawing...\n\n- a red, wooden ball: $P(W \\cap R)$\n- a blue ball made out of plastic $P(P \\cap B)$ \n\n## Contingency-table\n\nWe can easily represent all possible events in a contingency-table:\n\n |         | Red | Blue | Sum   |\n |---------|-----|------|-------|\n | Wood    | 4   | 6    | 10    |\n | Plastic | 8   | 12   | 20    |\n | Sum     | 12  | 18   | 30    |\n\n## Contingency-table\n\nDie nun berechneten Wahrscheinlichkeiten sind:\n\n |         | Red | Blue | Sum   |\n |---------|-----|------|-------|\n | Wood    | $P(R \\cap W)=\\frac{4}{30}$ | $P(B \\cap W)=\\frac{6}{30}$ | $P(W)=\\frac{10}{30}$|\n | Plastic | $P(R \\cap P)=\\frac{8}{30}$ | $P(B \\cap P)=\\frac{12}{30}$ | $P(P)=\\frac{20}{30}$ |\n | Sum     | $P(R)=\\frac{12}{30}$ |$P(B)=\\frac{18}{30}$ | |\n\n## Conditional probability\n\nWhile drawing a ball from the urn, we already feel that the ball is made of wood What is the probability it is of red color?\n\n$P(R|W) = \\frac{P(R \\cap W)}{P(W)} = \\frac{\\frac{4}{30}}{\\frac{1}{3}} = 0.8$\n\n# Random variable, sample, and sampling\n\n## Random variables\n\nA random variable is a variable that can take on a set of possible values, each with an distinct probability.\n\n**Example:** The random variable 'age' can take any value between 0 and $\\infty$, though a value of 35 is more likely than a value of 110. \n\n## Sample\n\nSimply speaking, a sample is a set of data points drawn from a population by a defined procedure. Mathematically speaking, a sample consists of a set of random variables. Each individual data point thereby represents a realisation of the many possible values of the random variable. The size of a sample is termed $n$.\n\n**Example:** If I pick one person ($n = 1$) in this class, a realisation of the random variable 'age' might be S = {26}. If I pick three persons ($n = 3$), the realizations might be S = {22, 28, 32}.\n\n## Sampling\n\nDrawing a sample from a population requires a defined procedure: \n\n- Simple random sampling\n- Systematic sampling\n- Stratified sampling\n- Clustered sampling\n\nAll those procedures have in common that each data point has a certain probability of inclusion in the sample. We then speak of a probability sampling design. There are also non-probability sampling designs.\n\n## Describing a sample\n\nSuppose that we sampled the DBH (diameter at breast height) of 150 pine trees randomly selected within a given forest.\n\nWe can inspect the sample using a histogram.\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=3, fig.width=3, fig.align='center'}\nset.seed(667) # One step ahead of the devil!\ndbh <- data.frame(Sample = 1:150, DBH = rnorm(150, 55, 15), Forest = rep(\"Forest A\", 150))\n\nggplot(dbh, aes(x = DBH)) + \n  geom_histogram(binwidth = 1 + log2(150), fill = \"grey\", col = \"black\") + \n  ylab(\"Count\") + \n  xlab(\"DBH [cm]\")\n```\n\nThe number of classes $k$ (bins) is estimated by: $k=1+log_{2}(n)$.\n\n## Describing the location and spread of a sample\n\n- Mean\n- Median\n- Mode\n- Variance\n- Standard deviation\n- Quantiles\n- Interquartile range\n- Skewness\n\n## From frequency to density\n\nSome DBH values were less frequent in our sample than other, suggesting that the likelihood of obtaining a certain DBH value followes a bell-shaped curve:\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=3, fig.width=9, fig.align='center'}\n\ngrid.arrange(\n  ggplot(dbh, aes(x = DBH)) + \n  geom_histogram(binwidth = 1 + log2(150), fill = \"grey\", col = \"black\") + \n  ylab(\"Count\") + \n  xlab(\"DBH [cm]\"),\n  \n  ggplot(dbh, aes(x = DBH, y = ..density..)) + \n  geom_histogram(binwidth = 1 + log2(150), fill = \"grey\", col = \"black\") + \n  ylab(\"Density\") + \n  xlab(\"DBH [cm]\"),\n  \n  ggplot(dbh, aes(x = DBH, y = ..density..)) + \n  geom_histogram(binwidth = 1 + log2(150), fill = \"grey\", col = \"black\") + \n    geom_density() +\n  ylab(\"Density\") + \n  xlab(\"DBH [cm]\"),\n  \n  ncol=3\n)\n```\n\nKnowing this curve would allow us to make statements about the forest we took the sample in (i.e., about the underlying population).\n\n# Probability distributions\n\n## Characteristics of probability distributions\n\nProbability distributions describe the likelihood of a random variable taking a certain value. Probability distributions have the following characteristics:\n\n- They are described through a probability density function (pdf)\n- The pdf has a set of parameters describing its location and variance\n- The integral between $a$ and $b$ is the probability that values between $a$ and $b$ occur\n- The complete area below the pdf is 1\n- All values of the pdf are independent\n- The pdf can be expressed as cumulative distribution function (cdf)\n\n## The normal distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\n\nx <- seq(-8,8,length.out=10000)\n\nset.seed(667)\n\nd1 <- data.frame(x=x,\n                v1=dnorm(x,mean=0,sd=1),\n                v2=dnorm(x,mean=2,sd=3),\n                v3=dnorm(x,mean=-4,sd=0.5))\nd1 <- melt(d1, id.vars=\"x\")\nd1$variable <- factor(as.character(d1$variable),labels=c(\"N(0,1)\",\"N(2,3)\",\"N(-4,0.5)\"))\n\nset.seed(667)\n\nd2 <- data.frame(x=x,\n                v1=pnorm(x,mean=0,sd=1),\n                v2=pnorm(x,mean=2,sd=3),\n                v3=pnorm(x,mean=-4,sd=0.5))\nd2 <- melt(d2, id.vars=\"x\")\nd2$variable <- factor(as.character(d2$variable), labels=c(\"N(0,1)\",\"N(2,3)\",\"N(-4,0.5)\"))\n\ngrid.arrange(\n  ggplot(d1, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"N(x)\") +\n    ggtitle(\"pdf\\n\"),\n  \n  ggplot(d2, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumN(x)\") +\n    ggtitle(\"cdf\\n\"),\n  \n  ncol=2\n)\n```\n\n$N(x) = P(X=x \\vert \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2*\\sigma^2}}$\n\n## Parameter estimation\n\nMany scientific conclusions are derived from the pdf of a population. For example, knowing the pdf of DBH values in our forests would allow us to calculate the verage yield we expect when harvesting the forest, or the amount of carbon stored in this particular forests. However, the parameters of a pdf are mostly unknown. We hence need to estimate those parameters from the sample.\n\nAssuming that the DBH is normally distributed in the population, we estimate: $\\hat{\\mu} = \\overline{x} = 55.5$ and a $\\hat{\\sigma^2} = s = 15.7$.\n\nThe estimates will approach the 'true' population value with $n \\to \\infty$. This means that larger samples will yield more precise estimates than smaller samples. However, the estimate of larger samples does not need to be more accurate!\n\n## Parameter estimation\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\n\ndat <- data.frame(DBH = seq(0, 120, length.out = 1000), \n                  pdf = dnorm(seq(0, 120, length.out = 1000), mean(dbh$DBH), sd = sd(dbh$DBH)), \n                  cdf = pnorm(seq(0, 120, length.out = 1000), mean(dbh$DBH), sd = sd(dbh$DBH)))\n\ngrid.arrange(\nggplot(dat, aes(x = DBH, y = pdf)) +\n  geom_line(),\nggplot(dat, aes(x = DBH, y = cdf)) +\n  geom_line(), ncol = 2)\n\n```\n\n## Standard error and confidence interval\n\nTo acount for uncertainty in the estimate, it is wise to estimate a range of values instead of a single value. In particular, if we would repeat the sampling of DBH values, we could yield a slightly different $\\hat{\\mu}$ each time, depending on the variance of the sample and the sample size (i.e., precision of the estimate). To express the variation in $\\hat{\\mu}$ when repeating the sampling, we make use of the standard error:\n\n$SE = \\frac{s}{\\sqrt{n}}$\n\n## Standard error and confidence interval\n\nFrom the standard error, we can estimate the confidence interval. Typically, we choose a 95% or 99% confidence interval. If we would repeat the sampling, there would be a 95% or 99% chance of including the 'true' population parameter. However, the confidence interval does not tell us anything about the sample already drawn!\n\n$CI_{95\\%} = \\overline{x} \\pm SE * Z$\n\nwith:\n\n$Z = \\Phi^{-1}(0.95) = 1.96$ and $\\Phi \\sim N(0, 1)$.\n\n## Standard error and confidence interval\n\nFor our example, the 95% confidence interval is:\n\n$CI_{95\\%} = \\frac{15.7}{\\sqrt{150}} * 1.96 = 2.5$\n\nHence, when repeating the sampling we would expect a mean DBH of 55.5 $\\pm$ 2.5 cm in 95% of the cases.\n\n## Standard error and confidence interval\n\nAssume that we took 100 more samples of varying sample sizes (10 to 1000 trees). Further assume that we were following able to measure all trees in our forest, coming to the conlusion that the 'true' parameters are: $\\mu = 55$ and a $\\sigma^2 = 15$.\n\n## Standard error and confidence interval\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\n\nmean <- c()\nci <- c()\n\nfor(i in as.integer(seq(10, 1000, length.out = 100))){\n  x <- rnorm(i, 55, 15)\n  mean <- c(mean, mean(x))\n  ci <- c(ci, (sd(x) / sqrt(i)) * 1.96)\n}\n\nggplot(data.frame(n = as.integer(seq(2, 1000, length.out = 100)), DBH = mean, CI = ci),\n       aes(x = n, y = DBH)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = DBH - CI, ymax = DBH + CI)) +\n  geom_hline(yintercept = 55, linetype = \"dashed\", col = \"red\")\n\n```\n\n# Distributions\n\n## The most important distributions\n\n- Normal distribution\n- Binomial distribution\n- Poisson distribution\n- Log-normal distribution\n- t-distribution\n- F-distribution\n\n## Normal distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\nx <- seq(-8,8,length.out=10000)\n\nset.seed(667)\n\nd1 <- data.frame(x=x,\n                v1=dnorm(x,mean=0,sd=1),\n                v2=dnorm(x,mean=2,sd=3),\n                v3=dnorm(x,mean=-4,sd=0.5))\nd1 <- melt(d1, id.vars=\"x\")\nd1$variable <- factor(as.character(d1$variable),labels=c(\"N(0,1)\",\"N(2,3)\",\"N(-4,0.5)\"))\n\nset.seed(667)\n\nd2 <- data.frame(x=x,\n                v1=pnorm(x,mean=0,sd=1),\n                v2=pnorm(x,mean=2,sd=3),\n                v3=pnorm(x,mean=-4,sd=0.5))\nd2 <- melt(d2, id.vars=\"x\")\nd2$variable <- factor(as.character(d2$variable), labels=c(\"N(0,1)\",\"N(2,3)\",\"N(-4,0.5)\"))\n\ngrid.arrange(\n  ggplot(d1, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"N(x)\") +\n    ggtitle(\"pdf\\n\"),\n  \n  ggplot(d2, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumN(x)\") +\n    ggtitle(\"cdf\\n\"),\n  \n  ncol=2\n)\n```\n\n\n$N(x) = P(X=x \\vert \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2*\\sigma^2}}$\n\n## Parameter of the normal distribution\n\nDomain: $x \\in \\mathbb{R} = (- \\infty,\\infty)$\n\nMean: $\\mu$\n\nVariance: $\\sigma^2$\n\n## Binomial distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\nn <- 10000\n\nset.seed(667)\n\nd1 <- data.frame(B=c(rbinom(n, 40, 0.7),\n                     rbinom(n, 40, 0.5),\n                     rbinom(n, 20, 0.5)),\n                Parameters=c(rep(\"Binom(0.7,40)\", length(rbinom(n, 40, 0.7))),\n                            rep(\"Binom(0.5,40)\", length(rbinom(n, 40, 0.7))),\n                            rep(\"Binom(0.5,20)\", length(rbinom(n, 40, 0.7)))))\n\nset.seed(667)\n\nd2 <- data.frame(x=c(1:40,1:40,1:40),\n                 B=c(pbinom(1:40,40,0.7),pbinom(1:40,40,0.5),pbinom(1:40,20,0.5)),\n                 Parameters=c(rep(\"Binom(0.7,40)\", length(pbinom(1:40,40,0.7))),\n                              rep(\"Binom(0.5,40)\", length(pbinom(1:40,40,0.5))),\n                              rep(\"Binom(0.5,20)\", length(pbinom(1:40,20,0.5)))))\n\ngrid.arrange(\n  \n  ggplot(d1, aes(x=B, fill=Parameters)) +\n  geom_bar(binwidth=1, position='dodge') +\n  scale_fill_brewer(palette=\"Set1\") +\n  ylab(\"Binom(k)\") +\n  xlab(\"k\") +\n  ggtitle(\"pdf\\n\"),\n\n  ggplot(d2, aes(x=x, y=B, col=Parameters)) +\n    geom_point() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumBinom(k)\") +\n    xlab(\"k\") +\n    ggtitle(\"cdf\\n\"),\n  \n  ncol=2\n)\n```\n\n$Binom(k) = (X=k \\mid p,n) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} p^{k}(1-p)^{n-k}$\n\n## Parameter of the binomial distribution\n\nDomain: $k \\in \\mathbb{N}_{0} = \\{0,1,2,3,...\\}$\n\nMean: $np$\n\nVariance: $np(1-p)$\n\n## Poisson distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\nn <- 10000\n\nset.seed(667)\n\nd1 <- data.frame(P=c(rpois(n, 2),\n                     rpois(n, 4),\n                     rpois(n, 10)),\n                Parameters=c(rep(\"Pois(0.7,40)\", length(rpois(n, 2))),\n                            rep(\"Pois(0.5,40)\", length(rpois(n, 4))),\n                            rep(\"Pois(0.5,20)\", length(rpois(n, 10)))))\n\nset.seed(667)\n\nd2 <- data.frame(x=c(1:20,1:20,1:20),\n                 P=c(ppois(1:20,2),ppois(1:20,4),ppois(1:20,10)),\n                 Parameters=c(rep(\"Pois(2)\", length(ppois(1:20,2))),\n                              rep(\"Pois(4)\", length(ppois(1:20,4))),\n                              rep(\"Pois(10)\", length(ppois(1:20,10)))))\n\ngrid.arrange(\n  \n  ggplot(d1, aes(x=P, fill=Parameters)) +\n  geom_bar(binwidth=1, position='dodge') +\n  scale_fill_brewer(palette=\"Set1\") +\n  ylab(\"Pois(k)\") +\n  xlab(\"k\") +\n  ggtitle(\"Verteilungsfunktion\\n\"),\n\n  ggplot(d2, aes(x=x, y=P, col=Parameters)) +\n    geom_point() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumPois(k)\") +\n    xlab(\"k\") +\n    ggtitle(\"Kummulative Verteilungsfunktion\\n\"),\n  \n  ncol=2\n)\n```\n\n\\begin{equation}\n  Pois(k) = P(X=k \\mid \\lambda) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!} \n\\end{equation}\n\n## Parameter of the poisson distribution\n\nDomain: $k \\in \\mathbb{N}_{0} = \\{0,1,2,3,...\\}$\n\nMean: $\\lambda$\n\nVariance: $\\lambda$ \n\n## Log-normal distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\nx <- seq(0,10,length.out=10000)\n\nset.seed(667)\n\nd1 <- data.frame(x=x,\n                v1=dlnorm(x,mean=1,sd=1),\n                v2=dlnorm(x,mean=1,sd=0.5),\n                v3=dlnorm(x,mean=1,sd=0.25))\nd1 <- melt(d1, id.vars=\"x\")\nd1$variable <- factor(as.character(d1$variable),labels=c(\"LN(1,1)\",\"LN(1,0.5)\",\"LN(1,0.25)\"))\n\nset.seed(667)\n\nd2 <- data.frame(x=x,\n                v1=plnorm(x,mean=1,sd=1),\n                v2=plnorm(x,mean=1,sd=0.5),\n                v3=plnorm(x,mean=1,sd=0.25))\nd2 <- melt(d2, id.vars=\"x\")\nd2$variable <- factor(as.character(d2$variable), labels=c(\"LN(1,1)\",\"LN(1,0.5)\",\"LN(1,0.25)\"))\n\ngrid.arrange(\n  ggplot(d1, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"N(x)\") +\n    ggtitle(\"Verteilungsfunktion\\n\"),\n  \n  ggplot(d2, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumN(x)\") +\n    ggtitle(\"Kumulative Verteilungsfunktion\\n\"),\n  \n  ncol=2\n)\n\n```\n\n$logN(x) = P(X=x \\vert \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}e^{-\\frac{(ln x-\\mu)^2}{2*\\sigma^2}}, x>0$\n\n## Parameter of the log-normal distribution\n\nDomain: $x \\in \\mathbb{R}_{0}^{+} = (0,\\infty)$\n\nMean: $e^{2\\mu+sigma^{2}/2}$\n\nVariance: $(e^{\\sigma^{2}-1})e^{2\\mu+sigma^{2}}$\n\n# Null hypothesis significance testing\n\n## Comparing samples\n\nSuppose we have sampled another 150 pine trees in another forest. We can compare both samples using boxplots.\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=3, fig.width=3, fig.align='center'}\ndbh2 <- data.frame(Sample = 1:150, DBH = rnorm(150, 48, 15), Forest = rep(\"Forest B\", 150))\n\nggplot(rbind(dbh, dbh2), aes(x = factor(Forest), y = DBH)) + \n  geom_boxplot() +  \n  xlab(\"\")\n```\n\n## Null hypothesis\n\nWe might ask following question: Are the mean DBH values the same among both forests? \n\nTo test this, we formulate a precise null hypothesis $H_{0}$: The mean DBH is equal among both samples, that is $\\overline{x}_{sp1} = \\overline{x}_{sp2}$\n\nThe principal idea behind NHST is to test how likely it would be to draw our sample at hand, assuming that $H_{0}$ is true. Hence, we want to know $P(D|H_{0})$. If $P(D|H_{0})$ is very small, we might safely reject $H_{0}$.\n\n## t-test\n\nThe NHST for two means is called a t-test (or Student's t-test). The basic idea behind the t-test is that the difference between two sample means would be exact zero if they were the same. Hence, under the assumption of $H_{0}$, a large difference is very unlikely.\n\n## t-test\n\nAs measure of difference in sample means, which also incorporates the standard deviation, we make use of the empirical t-value:\n\n$t = \\sqrt{\\frac{n_{sp1}*n_{sp2}}{n_{sp1}+n_{sp2}}}*\\frac{\\overline{x}_{sp1}-\\overline{x}_{sp2}}{S}$\n\nwith $\\overline{x}$ being the sample means, $n$ the sample size, and $S$ the weighted paired standard deviation defined as:\n\n$s^2 = \\frac{(n_{sp1}-1)*s_{sp1}^{2}+(n_{sp2}-1)*s_{sp2}^{2}}{n_{sp1}+n_{sp2}-2}$\n\nThe probability that the empirical t-value deviates from zero, given $H_{0}$ is true, is described by a t-distribution, which has only one parameter, the degree of freedom: $v=n_{sp1}+n_{sp2}-2$.\n\n## t-distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\nx <- seq(-6,6,length.out=10000)\n\nset.seed(667)\n\nd1 <- data.frame(x=x,\n                v1=dt(x,1),\n                v2=dt(x,4),\n                v3=dt(x,10))\nd1 <- melt(d1, id.vars=\"x\")\nd1$variable <- factor(as.character(d1$variable),labels=c(\"STUT(1)\",\"STUT(4)\",\"STUT(10)\"))\n\nset.seed(667)\n\nd2 <- data.frame(x=x,\n                v1=pt(x,1),\n                v2=pt(x,4),\n                v3=pt(x,10))\nd2 <- melt(d2, id.vars=\"x\")\nd2$variable <- factor(as.character(d2$variable), labels=c(\"STUT(1)\",\"STUT(4)\",\"STUT(10)\"))\n\ngrid.arrange(\n  ggplot(d1, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"STUT(x)\") +\n    ggtitle(\"pdf\\n\"),\n  \n  ggplot(d2, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumSTUT(x)\") +\n    ggtitle(\"cdf\\n\"),\n  \n  ncol=2\n)\n```\n\n$STUT(x) = (X=x \\mid v) = \\frac{B(\\frac{v+1}{2})}{\\sqrt{v*\\pi}B(\\frac{v}{2})}(1+\\frac{x^{2}}{v})^{-\\frac{v+1}{2}}$ \n\nWith $B(x,y)$ being the Euler Betafunction.\n\n## t-test\n\nLets calculate the t-value, degrees of freedom, and p-value for our example:\n\nThe sample means were 55.5 cm and 49.3 cm, respectively. Entering both in the equation for the t-value yields $t = 3.6$, with 298 degrees of freedom. Substracting the area between $[-t;+t]$ of the t-distribution from one yields $p=0.0004$. \n\nHence, the difference in means we observed in our samples is very unlikely under the assumption of $H_{0}$ being true. We thus might safely reject $H_{0}$ and conclude that the mean DBH values are significantely different between both samples.\n\n## The p-value\n\nThe p-value is the most loved and hated value in science. Whole scientific disciplines build upon the p-value in drawing their conclusions from data, most often taking an abritrary threshold of $p<0.05$ as indication to reject $H_{0}$.\n\n## The p-value\n\nEven though the p-value has a clear definition ($P(D|H_{0})$), it is very often misinterpreted. Here are the commen pitfalls:\n\n- The p-value does NOT tell you anything about the probability of the hypothesis given your data ($P(H_{0}|D)$)\n- The p-value is not a measure of effect size, that is it does NOT tell you anything about the strength of a difference\n- There is NO mathematical proof behind threshold (such as $p<0.05$) indicating significe\n- A smaller p-value is NO indication that a difference is more important than one with a larger p-value\n\nI you're interested in the abuse of p-values, please see: http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503\n\n## F-Test\n\nSimlar to the t-test, there is the F-test, which aims at testing on differences in the variances between two samples. The null hypothesis $H_{0}$ is: The sample variances are equal $s^{2}_{1} = s^{2}_{2}$.\n\nThe test statistic is the F-value, calculated as the ratio of the sample variances: \n\n$F = \\frac{s_{sp2}^{2}}{s_{sp1}^{2}}$. \n\nAssuming that $H_{0}$ is true, the F-value follows the F-distribution with two parameters:\n\n$v_{sp1}=n_{sp1}-1$ and $v_{sp2}=n_{sp2}-1$.\n\n## F-distribution\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=4, fig.width=10, fig.align='center'}\nx <- seq(0,4,length.out=1000)\n\nset.seed(667)\n\nd1 <- data.frame(x=x,\n                v1=df(x,2,2),\n                v2=df(x,2,5),\n                v3=df(x,10,10),\n                v4=df(x,10,5))\nd1 <- melt(d1, id.vars=\"x\")\nd1$variable <- factor(as.character(d1$variable),labels=c(\"F(2,2)\",\"F(2,5)\",\"F(10,10)\",\"F(10,5)\"))\n\nset.seed(667)\n\nd2 <- data.frame(x=x,\n                v1=pf(x,2,2),\n                v2=pf(x,2,5),\n                v3=pf(x,10,10),\n                v4=pf(x,10,5))\nd2 <- melt(d2, id.vars=\"x\")\nd2$variable <- factor(as.character(d2$variable), labels=c(\"F(2,2)\",\"F(2,5)\",\"F(10,10)\",\"F(10,5)\"))\n\ngrid.arrange(\n  ggplot(d1, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"F(x)\") +\n    ggtitle(\"pdf\\n\"),\n  \n  ggplot(d2, aes(x=x, y=value, col=variable)) +\n    geom_line() +\n    scale_color_brewer(palette=\"Set1\") +\n    ylab(\"KumF(x)\") +\n    ggtitle(\"cdf\\n\"),\n  \n  ncol=2\n)\n```\n\n$F(x) = (X=x \\mid v_{sp1}, v_{sp2}) = \\frac{\\sqrt{\\frac{(v_{sp1}x)^{v_{sp1}} v_{sp2}^{v_{sp2}}}{(v_{sp1}x+v_{sp2})^{v_{sp1}+v_{sp2}}}}}{xB(\\frac{v_{sp1}}{2},\\frac{v_{sp2}}{2})}$\n\n## F-Test\n\nThe sample variances in our example were 15.7 and 14.3, respectively. The empirical F-values is 0.91, with 149 degrees of freedom for both samples. Substracting the area between $[-F;+F]$ of the F-distribution from one yields $p=0.72$. \n\nThis results indicates that there is a high probability of obtaining a variance ratio similar to ours under $H_{0}$. We thus cannot safely reject $H_{0}$ and conclude that the variances are not significantely different.\n\n## Assumptions\n\nBoth tests, t-test and F-test, asumme that the samples are normally distributed. The t-test moreover assumes equal variances for both samples.\n\n# Correlation\n\n## Scatterplots\n\nWith our first sample, we also measured tree height:\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=3, fig.width=3, fig.align='center'}\nset.seed(665)\n\ndbh$Height <- unlist(lapply(dbh$DBH*1.3+40, function(x)rnorm(1, x, sd = 15)))\n\nggplot(dbh, aes(x = DBH, y = Height)) +\n  geom_point() +\n  ylim(50, 200)\n```\n\nTo quantify the strength of the relationship between DBH and height we make use of correlation analysis.\n\n## Correlation coefficient\n\nThe most commonly used correlation coefficient, the Pearson correlation coefficient, is defined as:\n\n$r_{xy} = \\frac{Cov(x,y)}{\\sigma(x) \\sigma(y)} = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(x_{i}-\\overline{x})^2}*\\sqrt{\\frac{1}{n}(y_{i}-\\overline{y})^2}}$\n\nand measures the linear monotonic increase in X when Y increases (or the other way round). It ranges from -1 to +1, where -1 indicates a perfect negative correlation, values close to zero indicate weak correlation, and +1 indicates perfect positive correlation.\n\n## Correlation coefficient\n\nCalculating the Pearson correlation coefficient for our examples yields $r_{xy}=0.8$, which indicates a strong positive relationship between DBH and height.\n\nFor ordinal data or for data that are not normally distributed there exist alaternative correlation coefficients, in particular the Spearman rank-correlation coefficients and Kendall's Tau.\n\n## Correlation coefficient\n\n```{r echo=FALSE, results='asis', warning=FALSE, message=FALSE, fig.height=5, fig.width=7.5, fig.align='center'}\nx <- runif(100,1,15)\na <- 0.8*x+rnorm(100,0,3)\nb <- -0.8*x+rnorm(100,0,4)\nc <- (x^6)/1000000+seq(0,2,length.out=100)+rnorm(100,0,0.5)\nd <- rep(10,100)+rnorm(100,0,0.5)\ne <- x+rnorm(100,0,0.1)\nf <- runif(100,1,15)\n\ncor <- melt(data.frame(x,a,b,c,d,e,f), id.vars=\"x\")\ncor$r <- NA\n\nfor(i in unique(cor$variable)){\n  cor[which(cor$variable==i),\"r\"] <- paste0(\"r=\",round(cor(cor[which(cor$variable==i),\"value\"],cor[which(cor$variable==i),\"x\"]),2))\n}\n\nggplot(cor, aes(x=x, y=value)) + \n  geom_point() +\n  geom_smooth(se=FALSE, col=\"red\", method=\"lm\") +\n  facet_wrap(~r, scales=\"free\") +\n  ylab(\"y\")\n```\n\n\n\n",
    "created" : 1476343964171.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3788884899",
    "id" : "946C5EE9",
    "lastKnownWriteTime" : 1476432826,
    "last_content_update" : 1476432826208,
    "path" : "~/Dropbox/Teching/Quantitative Methods/Recap/QuantMeth_Introduction_Slides.Rmd",
    "project_path" : "Recap/QuantMeth_Introduction_Slides.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}